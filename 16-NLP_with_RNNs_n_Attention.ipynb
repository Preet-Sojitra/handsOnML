{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85d357cc-b277-400b-9f03-ae7692484855",
   "metadata": {
    "id": "85d357cc-b277-400b-9f03-ae7692484855"
   },
   "source": [
    "# Natural Language Processing with RNNs and Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "411a3858-a1f3-4b6d-95cd-c5179ec68d82",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "411a3858-a1f3-4b6d-95cd-c5179ec68d82",
    "outputId": "4b802cc4-e82a-45e3-a40c-bc44611de239"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-24 00:03:40.118906: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-24 00:03:41.272056: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d94c99-0e0a-419a-82e8-8e50f9fb0d8b",
   "metadata": {
    "id": "e7d94c99-0e0a-419a-82e8-8e50f9fb0d8b"
   },
   "source": [
    "## Generating Shakespearean Text Using a Character RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d3715e-5007-4640-8ade-431983fcc106",
   "metadata": {
    "id": "22d3715e-5007-4640-8ade-431983fcc106"
   },
   "source": [
    "In 2015 blog post, Andrej Karpathy showed how RNN can be used to train a model to predict the next character in the sequence. We will look at how to build Char-RNN, step by step, starting with creation of dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc9c406-404c-4fd2-89c6-42db15f2f54a",
   "metadata": {
    "id": "4dc9c406-404c-4fd2-89c6-42db15f2f54a"
   },
   "source": [
    "### Creating the Training Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e41ebb3-9955-49e2-862f-0d87a70ba421",
   "metadata": {
    "id": "5e41ebb3-9955-49e2-862f-0d87a70ba421"
   },
   "source": [
    "First let's download all of Shakespere's work, using Keras's handy `get_file()` function and downloading the data from Andrej Karpathy's Char-RNN Project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7f5cc83-5b67-4093-9920-2860601d3f1b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "f7f5cc83-5b67-4093-9920-2860601d3f1b",
    "outputId": "2211e3d6-c802-444e-fc6c-d540ba97b697"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "1115394/1115394 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "filepath = keras.utils.get_file(\"shakespere.txt\", shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63adba24-7003-4979-8d73-717e7af08eed",
   "metadata": {
    "id": "63adba24-7003-4979-8d73-717e7af08eed",
    "outputId": "32fc0d44-ebf0-4b8d-c8e3-079fa06e8655"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(shakespeare_text[:148])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6ca72b-dae0-4d0c-b53f-3b00910d2056",
   "metadata": {
    "id": "de6ca72b-dae0-4d0c-b53f-3b00910d2056",
    "outputId": "1e9868cc-c127-444e-cd67-c3c6a17af8b3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n !$&',-.3:;?abcdefghijklmnopqrstuvwxyz\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join(sorted(set(shakespeare_text.lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ca28da-ba9e-450e-b406-a9ceed4bf5ad",
   "metadata": {
    "id": "97ca28da-ba9e-450e-b406-a9ceed4bf5ad",
    "outputId": "fcfa6586-0d06-450e-a484-f6ca98d9bd3f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"\".join(sorted(set(shakespeare_text.lower()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0ca15e-36af-4e3c-9cfc-298143579806",
   "metadata": {
    "id": "2c0ca15e-36af-4e3c-9cfc-298143579806"
   },
   "source": [
    "Next, we must encode every character as an integer. One option is to create a custom preprocessing layer as we did in Chapter-13. But in this case, it will be simpler to use Keras's `Tokenizer` class.\n",
    "\n",
    "First we need to fit a tokenizer to the text: it will find all the characters used in the text and map each of them to a different character ID, from the 1 to the number of distinct characters (it does not start at 0, as we can use that value for masking, as we will see later):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e10d736-8a4c-498d-af4a-fee973cc1733",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "0e10d736-8a4c-498d-af4a-fee973cc1733",
    "outputId": "f21edcb9-9a3d-4553-b65d-697fd948019e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True) # char_level=True means every character will be treated as a token\n",
    "tokenizer.fit_on_texts([shakespeare_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346ef3b9-21eb-4bee-bf45-c15b997d042a",
   "metadata": {
    "id": "346ef3b9-21eb-4bee-bf45-c15b997d042a"
   },
   "source": [
    "We set `char_level=True` to get the character-level encoding rather than the default word-level encoding. Note that tokenizer converts the text to lowercase by default (but we can set `lower=False` if we do not want that). Now the tokenizer can encode a sentence (or a list of sentences) to a list of character IDs and back, and it tells how many distinct characters are there and total number of characters in the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec78eb46-9492-4d6b-952e-2dcba00ded84",
   "metadata": {
    "id": "ec78eb46-9492-4d6b-952e-2dcba00ded84",
    "outputId": "c6273148-372f-4d28-f879-2e9164432239"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[20, 6, 9, 8, 3]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences([\"First\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d609e645-4553-4de5-934f-e0a2a71f9b7b",
   "metadata": {
    "id": "d609e645-4553-4de5-934f-e0a2a71f9b7b",
    "outputId": "f8d4ccc3-07bd-4cfb-8f7d-089a5aca18f2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f i r s t']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([[20,6,9,8,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4712937-9df8-4b83-84b9-cff28be633ce",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "d4712937-9df8-4b83-84b9-cff28be633ce",
    "outputId": "913dc65b-12cc-4904-b2ff-5d0c71708aa7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_id = len(tokenizer.word_index) # number of distinct characters\n",
    "max_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c893cccc-871f-4234-8a3d-368972ab71ed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "c893cccc-871f-4234-8a3d-368972ab71ed",
    "outputId": "32ebd427-7fcd-46c9-e74a-0b61c5ef95cf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_size = sum(tokenizer.word_counts.values()) # total number of characters\n",
    "dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a5e908-4ea3-452e-9bc1-d69e49c869d5",
   "metadata": {
    "id": "00a5e908-4ea3-452e-9bc1-d69e49c869d5",
    "outputId": "b0e7a065-e8e1-45ed-d71a-40198014cd0f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(shakespeare_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd5c7b8-f808-4928-8cba-499a76bf3310",
   "metadata": {
    "id": "3cd5c7b8-f808-4928-8cba-499a76bf3310"
   },
   "source": [
    "Let's encode the full text so each character is represented by its ID (we subtract 1 to get IDs from 0 to 38, rather than 1 to 39):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db7c0f35-1309-477c-967b-2bf18780c867",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "db7c0f35-1309-477c-967b-2bf18780c867",
    "outputId": "f1fb2ed7-802f-489f-b666-bb11a0d8cf44"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacdd275-59ca-4c01-9984-3c2999160d95",
   "metadata": {
    "id": "eacdd275-59ca-4c01-9984-3c2999160d95"
   },
   "source": [
    "Before we continue, we need to split the dataset into training, a validation and a test set. We can't just shuffle all the characters in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df45fb3-b763-4312-aed0-fa87820d9c66",
   "metadata": {
    "id": "9df45fb3-b763-4312-aed0-fa87820d9c66"
   },
   "source": [
    "### How to split a Sequential Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb6f31d-b8b8-48cb-9f70-56f747f816b4",
   "metadata": {
    "id": "8bb6f31d-b8b8-48cb-9f70-56f747f816b4"
   },
   "source": [
    "> Refer notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c3e630-e8ea-41e7-b737-91a1542e4c40",
   "metadata": {
    "id": "13c3e630-e8ea-41e7-b737-91a1542e4c40"
   },
   "source": [
    "Let's take the first 90% of the text for the training set (keeping the rest for the validation set and the test set), and create a `tf.data.Dataset` that will return each character one by one from this set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aecbdcfb-8144-47da-9adf-9068acf30002",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "aecbdcfb-8144-47da-9adf-9068acf30002",
    "outputId": "65a65638-8ccc-4745-dfe5-5055cc40c0b5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_size = dataset_size * 90 // 100\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6018daf-3cd3-41ca-872b-bc629a8150ba",
   "metadata": {
    "id": "d6018daf-3cd3-41ca-872b-bc629a8150ba"
   },
   "source": [
    "### Chopping the Sequential Dataset into Multiple Windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241a7818-0598-4bbd-b4ef-17c703fea6cd",
   "metadata": {
    "id": "241a7818-0598-4bbd-b4ef-17c703fea6cd"
   },
   "source": [
    "The training set now consists of a single sequence of over a million characters, so we can't just train the neural network directly on it: the RNN would be equivalent to a deep net with over a million layers, and we would have a single (very long) instance to train it. Instead, we will use dataset's `window()` method to convert this long sequence of characters into many smaller windows of text. Every instance in the dataset will be a fairly short substring of the whole text, and the RNN will be unrolled only over the length of these substrings. This is called *truncated backpropogation through time*.\n",
    "\n",
    "Let's call the `window()` method to create a dataset of short text windows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2aaef84f-3177-4c32-a8db-6675f34a9951",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "2aaef84f-3177-4c32-a8db-6675f34a9951",
    "outputId": "6b3ccdef-c899-4711-ff57-9c6575bd45f7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_steps = 100\n",
    "window_length = n_steps + 1 # target = input shifted 1 character ahead\n",
    "dataset = dataset.window(window_length, shift=1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74984dd0-30dc-43ab-8d67-e370b92d9719",
   "metadata": {
    "id": "74984dd0-30dc-43ab-8d67-e370b92d9719"
   },
   "source": [
    "**TIP:**\n",
    "\n",
    "We can try tuning `n_steps`: it is easier to train RNNs on shorter input sequences, but of course the RNN will not be able to learn any pattern longer than `n_steps`, so don't make it too small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6d7bb5-a75c-4a23-848a-de27fc79bd81",
   "metadata": {
    "id": "ab6d7bb5-a75c-4a23-848a-de27fc79bd81"
   },
   "source": [
    "By default `window()` method creates a nonoverlapping windows, but to get the largest possible training set we use `shift=1` so that the first window contains characters 0 to 100, the second contains 1 to 101, and so on. To ensure that all the windows are exactly 101 characters long (which will allow to create batches without having to do padding), we set `drop_remainder=True` (otherwise the last 100 windows will contain 100 characters, 99 characters, and so on down to 1 character).\n",
    "\n",
    "The `window()` creates a dataset that contains windows, each of which is also represented as dataset. It is *nested dataset*, analogous to a list of lists. This is useful when we want to transform each window by calling its dataset methods (e.g., to shuffle them or batch them). However, we cannot use a nested dataset directly for training, as our models will expect tensors as input, not datasets. So we must call the `flat_map()` method: it converts nested dataset into *flat dataset* (one that does not contain datasets). Moreover, the `flat_map()` method takes a function as an argument, which allows us to transform each dataset in the nested dataset before flattening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ac7b70-0066-4331-95f4-8e8d30e9f8da",
   "metadata": {
    "id": "44ac7b70-0066-4331-95f4-8e8d30e9f8da"
   },
   "outputs": [],
   "source": [
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b533f16-3f1a-4a6f-9176-2bab8b65b4c0",
   "metadata": {
    "id": "4b533f16-3f1a-4a6f-9176-2bab8b65b4c0"
   },
   "source": [
    "Notice that we call the `batch(window_length)` on each window: since all windows have exactly the same length, we will get a single tensor for each of them. Now the dataset contains consecutive windows of 101 characters each. Since GD works best when the instances in the training set are independet and indentically distributed , we need to shuffle these windows. Then we can batch the windows and seperate the inputs (the first 100 characters) from the target (the last character):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27705eec-166c-4d8c-bc6c-049aa2914185",
   "metadata": {
    "id": "27705eec-166c-4d8c-bc6c-049aa2914185"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[: ,1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf886e1-669e-421d-9f42-16918764bf7b",
   "metadata": {
    "id": "3bf886e1-669e-421d-9f42-16918764bf7b"
   },
   "source": [
    "As discussed in Chapter-13, categorical input features should generally be encoded, usually as one-hot vectors or as embeddings. Here we will encode each character using a one-hot vector because they are fairly few distinct characters (only 39):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de451cdf-2655-45b9-9d41-522d3b18ed5d",
   "metadata": {
    "id": "de451cdf-2655-45b9-9d41-522d3b18ed5d"
   },
   "outputs": [],
   "source": [
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch)\n",
    ")\n",
    "\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccaf99c-8c7a-4969-9bcc-d5ebcb854f8b",
   "metadata": {
    "id": "dccaf99c-8c7a-4969-9bcc-d5ebcb854f8b",
    "outputId": "e14f2fdf-c7c6-4ccc-8b0d-d96f8203c85e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 100, 39) (32, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-13 09:37:01.003966: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "for X_batch , Y_batch in dataset.take(1):\n",
    "    print(X_batch.shape, Y_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12391d5-98dc-4d0e-a5e5-22020b1f81fd",
   "metadata": {
    "id": "f12391d5-98dc-4d0e-a5e5-22020b1f81fd"
   },
   "source": [
    "That's it! Preparing the dataset was the hardest part. Now let's create the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab275edf-826b-47e2-ba5d-82144661cff8",
   "metadata": {
    "id": "ab275edf-826b-47e2-ba5d-82144661cff8"
   },
   "source": [
    "### Building and Training the Char-RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee27250-4be6-4257-a34b-0eb10c5998d3",
   "metadata": {
    "id": "4ee27250-4be6-4257-a34b-0eb10c5998d3"
   },
   "outputs": [],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(monitor=\"loss\", patience=5)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"char-rnn_model.keras\",\n",
    "                                                      monitor=\"loss\",save_best_only=True)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id], dropout=0.2,\n",
    "                    recurrent_dropout=0.2), # 20% dropout on both the inputs (dropout) and hidden state (recurrent_dropout) ... Remove recurrent_dropout in both current and below layers when training model on GPU\n",
    "    keras.layers.GRU(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation=\"softmax\")) # numbers of neurons in dense layer = max_id = 39, because we want to predict next character and since we want to predict the probability of next character we use the \"softmax\" activation function.\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c347c64f-a628-4ed8-816d-6bc9d3436945",
   "metadata": {
    "id": "c347c64f-a628-4ed8-816d-6bc9d3436945"
   },
   "outputs": [],
   "source": [
    "h = model.fit(dataset, epochs=15, callbacks=[early_stopping_cb, model_checkpoint_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed84a912-9040-43e5-9e3e-ef7f6d7adf6b",
   "metadata": {
    "id": "ed84a912-9040-43e5-9e3e-ef7f6d7adf6b"
   },
   "source": [
    "> Model training and inference is done on GPU and its weights have been saved in \"char-rnn_model.keras\" file. Access collab notebook from [here](https://colab.research.google.com/drive/1GVDdAc-b-ysUxoQ4zUFIYrwLJguLWS8Q?usp=sharing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c447ccc-380e-43f7-bcd1-43a12d74dde8",
   "metadata": {
    "id": "5c447ccc-380e-43f7-bcd1-43a12d74dde8"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"char-rnn_model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce8faef-e316-43f1-ad60-4b1265da0c19",
   "metadata": {
    "id": "3ce8faef-e316-43f1-ad60-4b1265da0c19"
   },
   "source": [
    "### Using the Char-RNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eec1a46-c479-48fe-a953-07f2b58621db",
   "metadata": {
    "id": "0eec1a46-c479-48fe-a953-07f2b58621db"
   },
   "source": [
    "Now we have a model that can predict the next character in text written by Shakespeare. To feed it some text, we first need to preprocess it like we did earlier, so let's create a function for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c304581b-0a70-44f7-b472-0b9d85942841",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "c304581b-0a70-44f7-b472-0b9d85942841",
    "outputId": "bb550e79-c246-4340-cea6-96f8430b3518"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess(texts):\n",
    "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
    "    # print(tf.one_hot(X, max_id))\n",
    "    return tf.one_hot(X, max_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816d9a14-86e1-4975-9833-8947960bd010",
   "metadata": {
    "id": "816d9a14-86e1-4975-9833-8947960bd010"
   },
   "source": [
    "Now let's use the model to predict the next letter in some text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6d5d21-c38d-451b-9744-3bd2c4c9c8bd",
   "metadata": {
    "id": "2d6d5d21-c38d-451b-9744-3bd2c4c9c8bd"
   },
   "outputs": [],
   "source": [
    "X_new = preprocess([\"How are yo\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c983e4-8a89-4980-8873-b56941745f46",
   "metadata": {
    "id": "b2c983e4-8a89-4980-8873-b56941745f46"
   },
   "outputs": [],
   "source": [
    "Y_pred = np.argmax(model(X_new), axis=-1)\n",
    "Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a522b28-ac98-439c-9bab-cec7df7b889b",
   "metadata": {
    "id": "8a522b28-ac98-439c-9bab-cec7df7b889b"
   },
   "outputs": [],
   "source": [
    "tokenizer.sequences_to_texts(Y_pred+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afeb695a-100a-4368-bed6-3b3161164404",
   "metadata": {
    "id": "afeb695a-100a-4368-bed6-3b3161164404"
   },
   "outputs": [],
   "source": [
    "tokenizer.sequences_to_texts(Y_pred+1)[0][-1] # last character"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6b8c95-9308-4da1-8cb1-3bd3adcd348c",
   "metadata": {
    "id": "fb6b8c95-9308-4da1-8cb1-3bd3adcd348c"
   },
   "source": [
    "Success! The model guessed right. Now let's use this model to generate new text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbaf433-13cb-46dd-bcbf-cfde0efb7352",
   "metadata": {
    "id": "5fbaf433-13cb-46dd-bcbf-cfde0efb7352"
   },
   "source": [
    "### Generating Fake Shakespearean Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6aea64-dbb8-4b1c-a3c2-e3b598d3746b",
   "metadata": {
    "id": "0d6aea64-dbb8-4b1c-a3c2-e3b598d3746b"
   },
   "source": [
    "To generate new text using the Char-RNN model, we could feed it some text, make the model predict the most likely next letter, add it at the end of the text, then give the extended text to the model to guess the next letter, and so on. But in practice, it often leads to same words being repeated over and over again.\n",
    "\n",
    "Instead we can pick the next character randomly, with probability equal to the estimated probability using Tensorflow's `tf.random.categorical()` function. This will generate more diverse text.\n",
    "\n",
    "The `categorical()` function samples random class indices, given the class log probabilities (logits). To have more control over the diversity of the generated text, we can divide the logits by a number called *temperature*, which we can tweak as we wish: a temperature close to 0 will favour high-probability characters, while a very high tempeature will give all characters an equal probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4854dc-6b29-4fb5-90d2-7c268f7eced5",
   "metadata": {
    "id": "4c4854dc-6b29-4fb5-90d2-7c268f7eced5"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "tf.random.categorical([[np.log(0.5), np.log(0.4), np.log(0.1)]], num_samples=5).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c77bc4-6843-4947-9892-64f7456ad297",
   "metadata": {
    "id": "00c77bc4-6843-4947-9892-64f7456ad297"
   },
   "source": [
    "Let's break it down above code!\n",
    "\n",
    "Imagine you have a bag of colored marbles, and each color represents a different category. Now, let's say you want to randomly pick marbles from this bag, but not all colors are equally likely to be picked. Some colors might be more likely than others.\n",
    "\n",
    "The `tf.random.categorical` function is like a magical machine that helps you do this. It's like asking the machine to pick marbles from the bag according to certain rules.\n",
    "\n",
    "Here's how it works:\n",
    "\n",
    "1. **Input**: You need to tell the machine about the bag of marbles and how likely each color is to be picked. In technical terms, you provide the machine with a list of numbers called \"logits\". These logits represent the probabilities of each category. For example, if you have three colors (red, green, blue), you might tell the machine that there's a 50% chance of picking red, 40% chance of picking green, and 10% chance of picking blue. But you provide these probabilities in a special way using logits.\n",
    "\n",
    "2. **Generate Samples**: Once the machine knows about the probabilities, you ask it to pick marbles from the bag. You tell it how many marbles you want it to pick. In technical terms, you specify the number of samples you want.\n",
    "\n",
    "3. **Output**: After you ask the machine to pick marbles, it gives you a list of numbers back. Each number represents the color of a marble it picked. These numbers are the indices of the categories you defined earlier. For example, if you have three colors and the machine gives you the numbers 0, 1, 0, 2, 1, it means it picked the first color, then the second color, then the first color again, then the third color, and finally the second color again.\n",
    "\n",
    "So, in simple terms, `tf.random.categorical` is like a machine that randomly picks items (categories) from a list (distribution) based on how likely you tell it each item is to be picked. It then gives you back the items it picked in the form of a list of numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7d7fd5-345e-4e5c-9519-3b719fefbe36",
   "metadata": {
    "id": "be7d7fd5-345e-4e5c-9519-3b719fefbe36"
   },
   "source": [
    "The following `next_char()` function uses above mentioned approach to pick the next character to add to the input text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "421ff8d7-c6cb-4501-8d36-efcd1f485774",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "421ff8d7-c6cb-4501-8d36-efcd1f485774",
    "outputId": "854668ed-ac1c-47e6-9364-7256f4d8dc3f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def next_char(text, temperature=1):\n",
    "    X_new = preprocess([text])\n",
    "    y_proba = model(X_new)[0, -1:, :] # all columns of last row\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
    "    return tokenizer.sequences_to_texts(char_id.numpy())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f41a997-29fd-49ac-9210-8fc7e911268e",
   "metadata": {
    "id": "6f41a997-29fd-49ac-9210-8fc7e911268e"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "next_char(\"how are yo\", temperature=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d01f8c-2be3-44fd-a9ac-ff7a5c49bb11",
   "metadata": {
    "id": "32d01f8c-2be3-44fd-a9ac-ff7a5c49bb11"
   },
   "source": [
    "Now, let's write a small function that will call next_char() to get the next character and applied it to given text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be8f1b8e-dece-491f-9c98-ebbb40856150",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "be8f1b8e-dece-491f-9c98-ebbb40856150",
    "outputId": "7038043b-08a2-4279-e0f3-2860210625e1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def complete_text(text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "      text += next_char(text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5adc5c5-567f-4714-bb5d-f9492104eb99",
   "metadata": {
    "id": "d5adc5c5-567f-4714-bb5d-f9492104eb99"
   },
   "source": [
    "Now we are ready to generate some text! Let's try with different temperatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56e44c9-4ab6-4b2d-8e08-14f32e36cca2",
   "metadata": {
    "id": "d56e44c9-4ab6-4b2d-8e08-14f32e36cca2"
   },
   "outputs": [],
   "source": [
    "print(complete_text(\"t\", temperature=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de089d3a-6755-4e86-996b-fbe76842d78e",
   "metadata": {
    "id": "de089d3a-6755-4e86-996b-fbe76842d78e"
   },
   "outputs": [],
   "source": [
    "print(complete_text(\"w\", temperature=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36f105c-219a-44d7-b21b-5d851be84c6d",
   "metadata": {
    "id": "e36f105c-219a-44d7-b21b-5d851be84c6d"
   },
   "outputs": [],
   "source": [
    "print(complete_text(\"w\", temperature=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c99904-80a1-41ed-8846-6f01ab967d51",
   "metadata": {
    "id": "a2c99904-80a1-41ed-8846-6f01ab967d51"
   },
   "source": [
    "Apparently our Shakespeare model works best at temperature close to 1. To generate more convincing text, we could try using more GRU layers and more neurons per layer, training for longer, and add some regularization (for example, we could set `recurrent_dropout=0.3` in `GRU` layers). Moreover, the model is currently incapable of learning patterns longer than `n_steps`, which is just 100 characters. We could try making this window larger, but it will also make training harder and even LSTM and GRU cells cannot handle very long sequences.\n",
    "\n",
    "Alernatively, we could use a stateful RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6033fbb6-0147-4430-b965-b75ee74364cf",
   "metadata": {
    "id": "6033fbb6-0147-4430-b965-b75ee74364cf"
   },
   "source": [
    "### Stateful RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a56087-ad38-47cc-a390-b4659631dfe0",
   "metadata": {
    "id": "81a56087-ad38-47cc-a390-b4659631dfe0"
   },
   "source": [
    "Until now, we have used only *stateless* RNNs: at each training iteration the model starts with a hidden state full of zeros, then it updates this state at each time step, and after the last time step, it throws it away, as it is not needed anymore. What if we told the RNN to preserve this final state after processing one training batch and use this as the initial state for the next training batch? This way model can learn long-term patterns despite only backpropogating through short sequences. This is called *stateful* RNN. Let's build one.\n",
    "\n",
    "First, note that the stateful RNN only makes sense if each input sequence in a batch starts exactly where the corresponding sequence in the previous batch left off. So the first thing we need to do build a stateful RNN is to use sequential and nonoverlapping input sequences (rather than shuffled and overlapping sequences we used to train stateless RNNs).\n",
    "\n",
    "When creating the `Dataset`, we must therefore use `shift=n_steps` rather than `shift=1` when calling `window()` method. Moreover, we must obviously not call the `shuffle()` method. Batching is much harder when preparing data for stateful RNN than it is for stateless RNN. Indeed, if we were to call `batch(32)`, then 32 consecutive windows would be put in the same batch, and the following batch would not continue each of these window where it left off. The first batch would contain window 1 to 32 and the second batch would contain windows 33 to 64, so if we consider, say, the first window of each batch (i.e., windows 1 and 33), we can say see that they are not consecutive. The simplest solution to this problem is to just use \"batches\" containing a single window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2NHQjrZ3YPX4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "2NHQjrZ3YPX4",
    "outputId": "55aa54e1-daaf-4f77-dd7e-8ae533345981"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
    "dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "dataset = dataset.batch(1)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zx8D1sHHcYIw",
   "metadata": {
    "id": "zx8D1sHHcYIw"
   },
   "source": [
    "Batching is harder but not impossible. For ex: we could chop Shakespeare's text into 32 texts of equal lengths, create one dataset of consecutive input sequences for each of them, and finally use\n",
    "`tf.train.Dataset.zip(datasets).map(lambda *windows:\n",
    "tf.stack(windows))` to create proper consecutive batches, where $n^{th}$ input sequence in a batch starts off exactly where the $n^{th}$ input sequence ended in the previous batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "hFSAwbR-B6u_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "hFSAwbR-B6u_",
    "outputId": "7e0424c0-307c-468e-e7b3-db8206973c1b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "encoded_parts = np.array_split(encoded[:train_size], batch_size)\n",
    "datasets = []\n",
    "for encoded_part in encoded_parts:\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(encoded_part)\n",
    "    dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "    datasets.append(dataset)\n",
    "dataset = tf.data.Dataset.zip(tuple(datasets)).map(\n",
    "    lambda *windows: tf.stack(windows))\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c_1uApeUdLwp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "c_1uApeUdLwp",
    "outputId": "41690b57-6673-4b41-e188-5ff9cda02689"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# we need to set `stateful=True` in every RL. Stateful RNN also needs to know\n",
    "# the batch size (since it will preserve a state for each input sequence in the\n",
    "# batch), so we must set `batch_input_shape` in the first layer. We can leave\n",
    "# second dimension unspecified, since inputs could have any length\n",
    "\n",
    "# commenting out `recurrent_dropout=0.2` so that we can use GPU acceleration\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, stateful=True, dropout=0.2,\n",
    "                    #  recurrent_droupout=0.2,\n",
    "                     batch_input_shape=[batch_size, None, max_id]),\n",
    "    keras.layers.GRU(128, return_sequences=True, stateful=True, dropout=0.2,\n",
    "                    #  recurrent_droupout=0.2\n",
    "                     ),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
    "                                                    activation=\"softmax\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Tztyth-ee4rl",
   "metadata": {
    "id": "Tztyth-ee4rl"
   },
   "source": [
    "At the end of each epoch, we need to reset the states before we got back to the beginning of the text. For this we can use small callback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cPraHsZUfD1E",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "cPraHsZUfD1E",
    "outputId": "d7e9a9cd-53dd-48e6-ad67-aabf8fce8e43"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class ResetStatesCallback(keras.callbacks.Callback):\n",
    "  def on_epoch_begin(self, epochs, logs):\n",
    "    self.model.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bs3mZhlcfPJA",
   "metadata": {
    "id": "bs3mZhlcfPJA"
   },
   "source": [
    "And now we can compile and fit the model (for more epochs, because each epoch is much shorter than earlier, and there is only one instance per batch):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6CYtETGLfdkF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6CYtETGLfdkF",
    "outputId": "694c4393-e44b-41bd-e442-2d025bf8ca5b",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "313/313 [==============================] - 12s 17ms/step - loss: 2.6243\n",
      "Epoch 2/100\n",
      "313/313 [==============================] - 5s 16ms/step - loss: 2.2412\n",
      "Epoch 3/100\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 2.1065\n",
      "Epoch 4/100\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 2.0313\n",
      "Epoch 5/100\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.9802\n",
      "Epoch 6/100\n",
      "313/313 [==============================] - 7s 21ms/step - loss: 1.9449\n",
      "Epoch 7/100\n",
      "313/313 [==============================] - 5s 14ms/step - loss: 1.9165\n",
      "Epoch 8/100\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 1.8954\n",
      "Epoch 9/100\n",
      "313/313 [==============================] - 5s 16ms/step - loss: 1.8773\n",
      "Epoch 10/100\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 1.8628\n",
      "Epoch 11/100\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.8513\n",
      "Epoch 12/100\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.8390\n",
      "Epoch 13/100\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 1.8293\n",
      "Epoch 14/100\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.8217\n",
      "Epoch 15/100\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.8140\n",
      "Epoch 16/100\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 1.8059\n",
      "Epoch 17/100\n",
      "313/313 [==============================] - 5s 17ms/step - loss: 1.8005\n",
      "Epoch 18/100\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 1.7948\n",
      "Epoch 19/100\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 1.7863\n",
      "Epoch 20/100\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.7843\n",
      "Epoch 21/100\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.7785\n",
      "Epoch 22/100\n",
      "313/313 [==============================] - 5s 16ms/step - loss: 1.7749\n",
      "Epoch 23/100\n",
      "313/313 [==============================] - 5s 16ms/step - loss: 1.7710\n",
      "Epoch 24/100\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 1.7670\n",
      "Epoch 25/100\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.7638\n",
      "Epoch 26/100\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.7608\n",
      "Epoch 27/100\n",
      "313/313 [==============================] - 5s 14ms/step - loss: 1.7569\n",
      "Epoch 28/100\n",
      "313/313 [==============================] - 5s 14ms/step - loss: 1.7545\n",
      "Epoch 29/100\n",
      "313/313 [==============================] - 5s 17ms/step - loss: 1.7507\n",
      "Epoch 30/100\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.7492\n",
      "Epoch 31/100\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 1.7459\n",
      "Epoch 32/100\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.7430\n",
      "Epoch 33/100\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 1.7393\n",
      "Epoch 34/100\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 1.7372\n",
      "Epoch 35/100\n",
      "313/313 [==============================] - 5s 17ms/step - loss: 1.7353\n",
      "Epoch 36/100\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 1.7342\n",
      "Epoch 37/100\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.7349\n",
      "Epoch 38/100\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 1.7289\n",
      "Epoch 39/100\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.7293\n",
      "Epoch 40/100\n",
      "313/313 [==============================] - 5s 17ms/step - loss: 1.7269\n",
      "Epoch 41/100\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 1.7246\n",
      "Epoch 42/100\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.7221\n",
      "Epoch 43/100\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.7208\n",
      "Epoch 44/100\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 1.7194\n",
      "Epoch 45/100\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.7179\n",
      "Epoch 46/100\n",
      "313/313 [==============================] - 5s 16ms/step - loss: 1.7175\n",
      "Epoch 47/100\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.7165\n",
      "Epoch 48/100\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 1.7162\n",
      "Epoch 49/100\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.7143\n",
      "Epoch 50/100\n",
      "313/313 [==============================] - 5s 17ms/step - loss: 1.7128\n",
      "Epoch 51/100\n",
      "313/313 [==============================] - 5s 16ms/step - loss: 1.7102\n",
      "Epoch 52/100\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 1.7089\n",
      "Epoch 53/100\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.7089\n",
      "Epoch 54/100\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 1.7047\n",
      "Epoch 55/100\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.7040\n",
      "Epoch 56/100\n",
      "313/313 [==============================] - 5s 14ms/step - loss: 1.7055\n",
      "Epoch 57/100\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.7058\n",
      "Epoch 58/100\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 1.7036\n",
      "Epoch 59/100\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 1.7011\n",
      "Epoch 60/100\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.7022\n",
      "Epoch 61/100\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 1.6995\n",
      "Epoch 62/100\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.7008\n",
      "Epoch 63/100\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 1.6983\n",
      "Epoch 64/100\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.6994\n",
      "Epoch 65/100\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 1.6950\n",
      "Epoch 66/100\n",
      "313/313 [==============================] - 5s 14ms/step - loss: 1.6964\n",
      "Epoch 67/100\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.6952\n",
      "Epoch 68/100\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 1.6943\n",
      "Epoch 69/100\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.6935\n",
      "Epoch 70/100\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 1.6932\n",
      "Epoch 71/100\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.6913\n",
      "Epoch 72/100\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 1.6894\n",
      "Epoch 73/100\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 1.6893\n",
      "Epoch 74/100\n",
      "313/313 [==============================] - 5s 17ms/step - loss: 1.6887\n",
      "Epoch 75/100\n",
      "313/313 [==============================] - 5s 14ms/step - loss: 1.6952\n",
      "Epoch 76/100\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.6892\n",
      "Epoch 77/100\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.6872\n",
      "Epoch 78/100\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 1.6879\n",
      "Epoch 79/100\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.6863\n",
      "Epoch 80/100\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 1.6859\n",
      "Epoch 81/100\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.6854\n",
      "Epoch 82/100\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 1.6852\n",
      "Epoch 83/100\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.6830\n",
      "Epoch 84/100\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 1.6832\n",
      "Epoch 85/100\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.6824\n",
      "Epoch 86/100\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.6817\n",
      "Epoch 87/100\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 1.6830\n",
      "Epoch 88/100\n",
      "313/313 [==============================] - 5s 17ms/step - loss: 1.6814\n",
      "Epoch 89/100\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 1.6790\n",
      "Epoch 90/100\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.6793\n",
      "Epoch 91/100\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.6783\n",
      "Epoch 92/100\n",
      "313/313 [==============================] - 5s 16ms/step - loss: 1.6765\n",
      "Epoch 93/100\n",
      "313/313 [==============================] - 5s 17ms/step - loss: 1.6809\n",
      "Epoch 94/100\n",
      "313/313 [==============================] - 5s 16ms/step - loss: 1.6763\n",
      "Epoch 95/100\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.6766\n",
      "Epoch 96/100\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.6779\n",
      "Epoch 97/100\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 1.6757\n",
      "Epoch 98/100\n",
      "313/313 [==============================] - 5s 17ms/step - loss: 1.6746\n",
      "Epoch 99/100\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.6743\n",
      "Epoch 100/100\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.6745\n"
     ]
    }
   ],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(monitor=\"loss\", patience=5)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    \"char-rnn_stateful_model.keras\", monitor=\"loss\",save_best_only=True)\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "h = model.fit(dataset, epochs=100,\n",
    "                    callbacks=[ResetStatesCallback(), early_stopping_cb,\n",
    "                               model_checkpoint_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8udwe2Qg7-T",
   "metadata": {
    "id": "b8udwe2Qg7-T"
   },
   "source": [
    "Once we have trained model, it will only be possible to use it to make predictions for batches of the same size as were used during training. To avoid this restriction, we need to create an identical *stateless* model, and copy the stateful model's weights to this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "WDrC4Pf0GoVp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "WDrC4Pf0GoVp",
    "outputId": "cece7164-ca5e-40ef-81b2-38d4fd239136"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we can get rid of dropout since it is used only during training\n",
    "\n",
    "stateless_model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id]),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
    "                                                    activation=\"softmax\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ctxek5sCHBRR",
   "metadata": {
    "id": "Ctxek5sCHBRR"
   },
   "source": [
    "To set the weights, we first need to build the model (so the weights get created):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29sdnH6FHKEv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "29sdnH6FHKEv",
    "outputId": "7a453481-08f9-4c5e-c00a-91f7fc2260a7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stateless_model.build(tf.TensorShape([None, None, max_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "PfY21FJcHPuj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "PfY21FJcHPuj",
    "outputId": "9933b1cb-5482-4b2d-87f5-485f19b4be2e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stateless_model.set_weights(model.get_weights())\n",
    "model = stateless_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ynbEZ7RgHXOv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "ynbEZ7RgHXOv",
    "outputId": "111c1764-9339-4e04-8001-06a86c380fbb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tity oud,\n",
      "dreamsof a his vows want beaugh thon to m\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "print(complete_text(\"t\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YKslV4ZzHpTj",
   "metadata": {
    "id": "YKslV4ZzHpTj"
   },
   "source": [
    "Now that we have built a character-level model, it's time to look at word-level models and tackle a common NLP task: *sentiment analysis*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a989482-6278-4c9b-8108-b31a83ed6776",
   "metadata": {
    "id": "D5tfMlZ0H-JZ"
   },
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009e835d-dada-417e-bc36-727b4815ed0d",
   "metadata": {},
   "source": [
    "We will work with IMDb reviews dataset. It consists of 50,000 reviews (25000 for training and 25000 for testing), along with simple binary target for each review indicating whether it is negative (0) or positive (0). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0795403-c198-4de9-b085-c5cc5cc9bf31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "\u001b[1m17464789/17464789\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1us/step\n"
     ]
    }
   ],
   "source": [
    "# loading dataset\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.imdb.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12b0b0e-57a7-4cb6-9c5d-a1a7e39a5874",
   "metadata": {},
   "source": [
    "`X_train` is a 2D list. Each list is one review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "148b23fb-8336-48f2-843f-0e5f3a3cb128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dce5d4-233f-44de-bf19-88086bcc328b",
   "metadata": {},
   "source": [
    "Where are the movie reviews? Well, the dataset is already preprocessed for us: `X_train` consists of a list of reviews, each of which is represented by Numpy array of integers, where each integer represents a word. All punctuation was removed and then words were converted to lowercase, split by spaces, and finally indexed by frequencies (so low integers corresponds to frequent words). The integers 0, 1 and 2 are special: they represent the padding token, the *start-of-sequence* (SSS) token and unknown words respectively. \n",
    "\n",
    "To visualize the review, we can decode it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7697e7e-df16-45f9-a04c-df935fc7fb5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "\u001b[1m1641221/1641221\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "word_index = keras.datasets.imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef24e4c4-e40b-4398-a753-f2c5eb48ba25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first few items from word_index\n",
    "def print_dict(dict, values=5):\n",
    "    count = 0\n",
    "    for key, value in dict.items():\n",
    "        print(key, \":\", value)\n",
    "        count += 1\n",
    "        if count >= values:  # Change 5 to the number of items you want to print\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa4b1fc9-20d6-45c7-86dd-0a96377ecbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fawn : 34701\n",
      "tsukino : 52006\n",
      "nunnery : 52007\n",
      "sonja : 16816\n",
      "vani : 63951\n"
     ]
    }
   ],
   "source": [
    "print_dict(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a023278-c238-444c-9ca5-3336b2a4b626",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_word = {id_ + 3: word for word, id_ in word_index.items()} # we are adding 3 to every id since first 3 ids are special one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b43cf531-2f7a-467b-9103-c81a7b5f06c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34704 : fawn\n",
      "52009 : tsukino\n",
      "52010 : nunnery\n",
      "16819 : sonja\n",
      "63954 : vani\n"
     ]
    }
   ],
   "source": [
    "print_dict(id_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "206e6f81-f229-4e01-8fe1-e952c048cecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are adding 3 special characters to the id_to_word dict\n",
    "for id_ , token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
    "    id_to_word[id_] = token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f1d3e505-92c6-4612-a1af-0c91ff950869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>',\n",
       " 'this',\n",
       " 'film',\n",
       " 'was',\n",
       " 'just',\n",
       " 'brilliant',\n",
       " 'casting',\n",
       " 'location',\n",
       " 'scenery',\n",
       " 'story']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[id_to_word[id_] for id_ in X_train[0][:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a36c8115-da95-4712-a2a4-7f6a6b7907c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sos> this film was just brilliant casting location scenery story'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join([id_to_word[id_] for id_ in X_train[0][:10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f15a1e-9fd8-405e-9c80-0fa879fe3fb9",
   "metadata": {},
   "source": [
    "In real project, we will have to preprocess the text ourself. We can do that using the `Tokenizer` class we used earlier, but this time setting `char_level=False` (which is default). When encoding words, it filters out a lot of characters, including most punctutations, line breaks and tabs (but we can change this setting by the `filters` argument). This is OK for English and many other scripts that use spaces between words, but not all scripts use spaces this way. Even in English, spaces are not always the best way to tokenize text: think of \"San Francisco\" or \"#ILoveDeepLearning\".\n",
    "\n",
    "Forunately there are better options. \n",
    "> Refer notes for better options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d54a4d-4ff4-4d79-bcd3-592fb6b59b4b",
   "metadata": {},
   "source": [
    "If we want to deploy our model to mobile devices or a web browser, and we don't want to have write a different preprocessing function every time, then we will want to handle preprocessing using only TF operations, so it can be included in model itself. Let's see how.\n",
    "\n",
    "Let's load the original IMDb reviews as text (byte strings), using Tensorflow Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "37826838-f3ca-4175-9662-6ae00a82b232",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)\n",
    "train_size = info.splits[\"train\"].num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3c4d3d-c367-44a0-a6fa-f991be3866b2",
   "metadata": {},
   "source": [
    "Next, let's write preprocessing function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eaa7dbe3-05be-4061-9f91-7939b355a112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X_batch, y_batch):\n",
    "    X_batch = tf.strings.substr(X_batch, 0, 300)\n",
    "    X_batch = tf.strings.regex_replace(X_batch, b\"<br\\\\s*/?>\", b\" \")\n",
    "    X_batch = tf.strings.regex_replace(X_batch, b\"[^a-zA-Z']\", b\" \")\n",
    "    X_batch = tf.strings.split(X_batch)\n",
    "    return X_batch.to_tensor(default_value=b\"<pad>\"), y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9652375d-8580-4d92-b5a9-cfcfe88dbcad",
   "metadata": {},
   "source": [
    "It starts by truncating the reviews, keeping only the first 300 characters of each: this will speed up training, and it won't impact performance too much because we can generally tell whether a review is positive or not in the first sentence or two. Then it uses *regular expressions* to replace `<br/>` with spaces, and to replace any characters other than letters and quotes with spaces. For example, the text `\"Well, I can't<br />\"` will become `\"Well, I can't\"`. Finally the `preprocess()` function splits the reviews by spaces, which returns ragged tensor, and it converts this ragged tensor to a dense tensor, padding all reviews with padding token `\"<pad>\"` so that they all have same length. \n",
    "\n",
    "Next, we need to construct the vocabulary. This requires going through the whole training set once, applying our `preprocess()` function, and using a `Counter` to counter number of occurences of each word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "06dd0406-5020-4051-86c4-63e0fbcbe379",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-14 21:50:21.724773: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "vocabulary = Counter()\n",
    "for X_batch, y_batch in datasets[\"train\"].batch(32).map(preprocess):\n",
    "    for review in X_batch:\n",
    "        vocabulary.update(list(review.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2355530f-ed4d-4422-91a6-9588ab00df5d",
   "metadata": {},
   "source": [
    "Let's look at the three most common words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2a1a0c17-8617-4eeb-9c32-179509609aae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b'<pad>', 214309), (b'the', 61137), (b'a', 38564)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary.most_common()[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a77aa3-7839-4554-8e64-cc26cc73fc0a",
   "metadata": {},
   "source": [
    "Great! We probably don't we need our model to know all the words in the dictionary to get good performance, though, so let's truncate the vocabulary, keeping only the 10,000 most common words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a5bfd080-2cd8-40f3-ac39-e76d3b7fa98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=10000\n",
    "truncated_vocabulary = [word for word, count in vocabulary.most_common()[:vocab_size]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc1d2ba-4867-4cdd-a6a1-750beab864a9",
   "metadata": {},
   "source": [
    "Now we need to add a preprocessing step to replace each word with its ID (i.e., its index in the vocabulary). Just like we did in Chapter-13, we will create a lookup table for this, using 1000 out-of-vocabulary (oov) buckets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5d92d121-a526-4929-a0e1-50874b885c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tf.constant(truncated_vocabulary)\n",
    "word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)\n",
    "vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
    "num_oov_buckets = 1000\n",
    "table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66c1c67-daeb-4b3c-a4ea-af2d252d2e41",
   "metadata": {},
   "source": [
    "We can then use this table to look up the IDs of a few words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cde4b8c7-eb53-40b7-9281-4c8bb54f7fe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[   22,    12,    11, 10791]])>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.lookup(tf.constant([b\"This movie was faaaaantastic\".split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16412cbc-07fe-442e-90bd-05cde080e09a",
   "metadata": {},
   "source": [
    "Note that the words \"this\", \"movie\", \"was\" were found in the table, so their IDs are lower than 10,000, while the word \"faaaaantastic\" was not found, so it was mapped to one of the oov buckets, with an ID greater than or equal to 10,000. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db405022-a98e-4a83-b09f-b6e0dd8adcaa",
   "metadata": {},
   "source": [
    "**TIP:**\n",
    "\n",
    "TF Transform (introduced in Chapter 13) provides some useful functions to handle\n",
    "such vocabularies. For example, check out the\n",
    "`tft.compute_and_apply_vocabulary()` function: it will go through the dataset to\n",
    "find all distinct words and build the vocabulary, and it will generate the TF\n",
    "operations required to encode each word using this vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfedad01-388c-42bd-8d38-88802e1c6a7f",
   "metadata": {},
   "source": [
    "Now we are ready to create the final training set. We batch the reviews, then convert them to short sequences of words using the `preprocess()` function, then encode these words using a simple `encode_words()` function that uses the table we just built, and finally prefetch the next batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4061e7d6-f826-472e-9286-6f61f7de9fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_words(X_batch, y_batch):\n",
    "    return table.lookup(X_batch), y_batch\n",
    "\n",
    "train_set = datasets[\"train\"].batch(32).map(preprocess)\n",
    "train_set = train_set.map(encode_words).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2773173d-aa70-4718-a8bb-64600ab390f3",
   "metadata": {},
   "source": [
    "At last we can create the model and train it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "36d268cf-40aa-45d0-b1bd-c9175456a431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 114ms/step - accuracy: 0.5451 - loss: 0.6740\n"
     ]
    }
   ],
   "source": [
    "embed_size = 128\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size, input_shape=[None]),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.GRU(128),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "h = model.fit(train_set, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc494ab-b5a2-4790-8791-690ecc5ebb33",
   "metadata": {},
   "source": [
    "> Will be trained on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14f5405-192e-4c01-910f-d496bdccbee7",
   "metadata": {},
   "source": [
    "The first layer is the `Embedding` layer, which will convert word IDs into embeddings. The embedding matrix needs to have one row per word ID (vocab_size + num_oov_buckets) and one column per embedding dimension (here we have used 128 dimensions, but this is hyperparameter that we can tune). Whereas the inputs of the model will be 2D tensors of shape [*batch_size*, *time_steps*], the output of the `Embedding` layer will be a 3D tensor of shape [*batch size, time steps, embedding size*]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b768ef-ee2d-40a6-92d7-2922578cd71b",
   "metadata": {},
   "source": [
    "### Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2038e716-16d7-487a-a17c-169d4ae48273",
   "metadata": {},
   "source": [
    "As it stands, the model will need to learn that the padding tokens should be ignored. But we already know that! We need to tell the model to ignore that so that it can focus on the data that actually matters. It's actually trivial: simply add `mask_zero=True` when creating `Embedding` layer. This means that padding tokens (whose ID is 0) will be ignored by all downstream layers.\n",
    "\n",
    "> It is good idea to give 0 ID to the padding tokens. Here padding token are the most frequent words so they have 0 ID, but if they are not frequent, then we should make ID of it as 0.\n",
    "\n",
    "That's all!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c86b7ee-e3d1-41df-ba27-345db60080f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 128\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size, mask_zero=True, input_shape=[None]),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.GRU(128),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "h = model.fit(train_set, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f04dbb-7ab0-493e-b56e-ca74b2ef3109",
   "metadata": {},
   "source": [
    "Working:\n",
    "\n",
    "The `Embedding` layer creates a *mask tensor* equal to `K.not_equal(inputs, 0)` (where `K=keras.backend`): it is a Boolean tensor with same shape as inputs and it is equal to `False` anywhere the word IDs are 0, or `True` otherwise. This mask tensor is then automatically propogated by the model to all subsequent layers, as long as the time dimensions are preserved. So in this example, both `GRU` layers will receive the mask automatically, but since the second `GRU` layer does not return sequences (it only returns the outputs of the last time step), the mask will not be transmitted to the `Dense` layer. Each layer may handle the mask differently, but in general they simply ignore the masked time steps (i.e., time steps for which the mask is `False`). For example, when a RL encounters a masked time step, it simply copies the output from the previous time step. \n",
    "\n",
    "**Warning:**\n",
    "\n",
    "The LSTM and GRU layers have an optimized implementation for GPUs, based on\n",
    "Nvidias cuDNN library. However, this implementation does not support masking. If\n",
    "your model uses a mask, then these layers will fall back to the (much slower) default\n",
    "implementation.\n",
    "\n",
    "\n",
    "All the layers that receives the mask must support masking (or else an exception will be raised). This includes all RL, as well as `TimeDistributed` layer and few other layers. Any layer that supports masking must have an `support_masking` attribute equal to `True`.\n",
    "\n",
    "If we want to implement our own custom layer with masking support, we should add a `mask` argument to `call()` method. Additionally, we should set `self.support_masking=True` in the constructor. If the layer does not start with an `Embedding` layer, we may use the `keras.layers.Masking` layer instead: it sets the mask to `K.any(K.not_equal(inputs,0), axis=-1)`, meaning that time steps where the last dimension is full of zeros will be masked out in subsequent layers (again, as long as the time dimension exists). \n",
    "\n",
    "Using masking layers and automatic mask propogation works best for simple `Sequential` methods. It will not always work for more complex models, such as when you need to mix `Conv1D` layers with recurrent layers. In such cases, you will need to explicitly compute the mask and pass it to the appropriate layers, using it either the Functional API or the Subclassing API. For ex: the following model is identical to the previous model, except it is build using the Functional API and handles masking manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95107210-cc80-4a5a-833b-0963ae6285a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = keras.backend\n",
    "inputs = keras.layers.Input(shape=[None])\n",
    "mask = keras.layers.Lambda(lambda inputs: K.not_equal(inputs, 0))(inputs)\n",
    "z = keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size)(inputs)\n",
    "z = keras.layers.GRU(128, return_sequences=True)(z, mask=mask)\n",
    "z = keras.layers.GRU(128)(z, mask=mask)\n",
    "outputs = keras.layers.Dense(1, activation=\"sigmoid\")(z)\n",
    "model = keras.Model(inputs=[inputs], outputs=[outputs])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da8b12f-6dc7-4996-afab-afc1ce46523e",
   "metadata": {},
   "source": [
    "After training for few epochs, this model will become quite good at judging whether a review is positive or not. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7be045c-6976-448c-ae0a-86ea4fff322d",
   "metadata": {},
   "source": [
    "### Reusing Pretrained Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f7c504-a8de-47c0-852f-4bf9beb559bd",
   "metadata": {},
   "source": [
    "The TensorFlow Hub project makes it easy to reuse pretrained model components in our own models. These model components are called *modules*. Simply browse [TF Hub Repository](https://tfhub.dev/), find the one that we need, and copy the code example into our project and the module will be automatically downloaded, along with pretrained weights. Easy!\n",
    "\n",
    "For ex: let's use the `nnlm-en-dim50` sentence embedding module, version 1, in our sentiment analysis model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2225fa-4fd0-4d4f-a703-dabd6d8206b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "model = keras.Sequential([\n",
    "    hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1\", dtype=tf.string,\n",
    "                  input_shape=[], output_shape=[50]),\n",
    "    keras.layers.Dense(128, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351d4f5f-c622-4f37-aa38-f666b852543b",
   "metadata": {},
   "source": [
    "The `hub.KerasLayer` downloads the module from the given URL. This particular module is *sentence encoder*: it takes string as input and encodes each one as a single vector (in this case, a 50-dimensional vector). Internally, it parses the string (splitting words on spaces) and embedds each word using an embedding matrix that was pretrained on a huge corpus: the Google News 7B corpus. Then it computes the mean of all the word embeddings and the result is the sentence embedding. We can then add two simple `Dense` layers to create a good sentiment analysis model. By default, a `hub.KerasLayer` is not trainable, but we can set `trainable=True` when creating it to change that so that we can fine-tune it for our task.\n",
    "\n",
    "Next, we can just load the IMDb reviews dataset - no need to preprocess it (except for batching and prefetching) - and directly train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a357a9e-a8bb-428b-b325-08f044f9468e",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)\n",
    "train_size = info.splits[\"train\"].num_examples\n",
    "batch_size=32\n",
    "train_set = datasets[\"train\"].batch(batch_size).prefetch(1)\n",
    "h = model.fit(train_set, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8668ae-2e3b-451c-afea-bf0d5b3fb9a0",
   "metadata": {},
   "source": [
    "By default, TF Hub will cache the downloaded files into\n",
    "the local systems temporary directory. You may prefer to download them\n",
    "into a more permanent directory to avoid having to download them again\n",
    "after every system cleanup. To do that, set the `TFHUB_CACHE_DIR`\n",
    "environment variable to the directory of your choice (e.g.,\n",
    "`os.environ[\"TFHUB_CACHE_DIR\"] = \"./my_tfhub_cache\"`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1b6649-af93-4ca0-8c50-2e2d823e8b36",
   "metadata": {},
   "source": [
    "So far, we have looked at time series, text generation using Char-RNN, and sentiment analysis using word-level RNN models, training our own word embeddings or using pretrained embeddings. \n",
    "\n",
    "Let's now look at another important NLP task: *neural machine translation* (NMT), first using a pure Encoder-Decoder model, then improving it with attention mechanisms and finally looking the extraordinary Transformer architecture. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace12ce6-36c7-4fe9-86f2-3086f80b77b9",
   "metadata": {},
   "source": [
    "## An Encoder-Decoder Model for Neural Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b27910-d8c2-4832-b128-ee44fbb7db30",
   "metadata": {},
   "source": [
    "The Tensorflow Addons project includes many seq-to-seq tools to let us easily build production-ready Encoder-Decoders. For example, the following code creates a basic Encoder-Decoder model, similar to one represented in the figure:\n",
    "> Figure drawn in notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92ba5f8-df8a-43a0-8df1-29c74d503583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "\n",
    "vocab_size = 100\n",
    "embed_size = 10\n",
    "\n",
    "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "sequence_lengths = keras.layers.Input(shape=[], dtype=np.int32)\n",
    "\n",
    "embeddings = keras.layers.Embedding(vocab_size, embed_size)\n",
    "encoder_embeddings = embeddings(encoder_inputs)\n",
    "decoder_embeddings = embeddings(decoder_inputs)\n",
    "\n",
    "encoder = keras.layers.LSTM(512, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_embeddings)\n",
    "encoder_state = [state_h, state_c]\n",
    "\n",
    "sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "\n",
    "decoder_cell = keras.layers.LSTMCell(512)\n",
    "output_layer = keras.layers.Dense(vocab_size)\n",
    "decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell, sampler, output_layer=output_layer)\n",
    "final_outputs, final_state, final_sequence_lengths = decoder(decoder_embeddings, \n",
    "                                                             initial_state=encoder_state,\n",
    "                                                            sequence_length=sequence_lengths)\n",
    "Y_proba = tf.nn.softmax(final_outputs.rnn_output)\n",
    "\n",
    "model = keras.Model(inputs=[encoder_inputs, decoder_inputs, sequence_lengths], outputs=[Y_proba])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84806593-c4f7-4848-97ab-d505047799d9",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "First, we set `return_state=True` when creating LSTM layer so that we can get its final hidden state and pass it to decoder. Since we are using an LSTM Cell, it actually returns two hidden states (short term and long term). The `TrainingSampler` is one of several samplers available in Tensorflow Addons: their role is to tell the decoder at each step what is should pretend the previous output was. During inference, this should be the embedding of the previous target token: this is why we used `TrainingSampler`. In practice, it is often good idea to start training with the embedding of the target of the previous step and gradually transition to using the embedding of the actual token that was output at the previous step. This idea was introduced in 2015 paper. The `ScheduledEmbeddingTrainingSampler` will randomly choose between the target or the actual output, with probability that we can gradually change during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12291944-7af7-425e-9e60-a9f231bc084d",
   "metadata": {},
   "source": [
    "### Bidirectional RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5a2621-887e-4f34-bf92-d230d36af539",
   "metadata": {},
   "source": [
    "To implement this, run two recurrent layers on the same inputs, one reading the words from left to right  and the other reading them from right to left. Then simply combine their outputs at each time step, typically by concatenating them. This is called *bidirectional recurrent layer*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335c91db-52ca-47b4-b234-a4dc95e01a5f",
   "metadata": {},
   "source": [
    "To implement a bidirectional recurrent layer, wrap a recurrent layer in Keras, wrap a recurrent layer in `keras.layers.Bidirectional` layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29551952-dd02-4a3c-b4f1-0925e4eb2794",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.layers.Bidirectional(keras.layers.GRU(10, return_sequences=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f5a77e-4702-4e10-8f67-377386abd695",
   "metadata": {},
   "source": [
    "**NOTE:**\n",
    "\n",
    "The `Bidirectional` layer will create a clone of `GRU` layer (but in reverse direction), and it will run both and concatenate their outputs. So although the `GRU` layer has 10 units, the `Bidirectional` layer will output 20 values per time step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d2ba25-7930-424d-bbab-d0bf3f717a6e",
   "metadata": {},
   "source": [
    "### Beam Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921f404a-3581-4a0d-9840-e0638bb8faef",
   "metadata": {},
   "source": [
    "We can implement beam search fairly easily using Tensorflow Addons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af2c64a-77b2-4e3e-aba5-7f79c5866e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_width = 10\n",
    "\n",
    "decoder = tfa.seq2seq.beam_search_decoder.BeamSearchDecoder(cell=decoder_cell, beam_width=beam_width,\n",
    "                                                           output_layer=output_layer)\n",
    "decoder_initial_state = tfa.seq2seq.beam_search_decoder.tile_batch(encoder_state, \n",
    "                                                                  multiplier=beam_width)\n",
    "outputs, _, _ = decoder(embedding_decoder, start_tokens=start_tokens, end_token=end_token,\n",
    "                       initial_state=decoder_initial_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a04f686-fcc2-48ae-9d50-9211024043e6",
   "metadata": {},
   "source": [
    "We first create a `BeamSearchDecoder`, which wraps all decoder clones (in this case 10 clones). Then we create one copy of the encoder's final state for each decoder clone, and we pass these states to the decoder, along with start and end tokens.\n",
    "\n",
    "With all this, we can get good translations for fairly short sequences (especially if we use pretrained word embeddings). Unfortunately, this model will be really bad at translating long sentenes. *Attention mechanisms* are good game-changing innovation that addressed this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a7821c-862d-4464-b5ff-ec16a2e260ba",
   "metadata": {},
   "source": [
    "## Attention Mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f73c3f8-a373-4d13-b685-76c3e11b25b4",
   "metadata": {},
   "source": [
    "#### Bahdanau Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32aa8904-44bb-4632-8b57-ac2aeb61e6cb",
   "metadata": {},
   "source": [
    "#### Luong Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4e7c3c-57c0-4ed8-bb6f-cf60d95fcba0",
   "metadata": {},
   "source": [
    "Here's how we can add Luong attention to Encoder-Decoder model using Tensorflow addons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9241333-c292-43cc-9ce7-02696bd49cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mechanism = tfa.seq2seq.attention_wrapper.LuongAttention(units, encoder_state, \n",
    "                                      memeory_sequence_length=encoder_sequence_length)\n",
    "attention_decoder_cell = tfa.seq2seq.attention_wrapper.AttentionWrapper(\n",
    "                decoder_cell, attention_mechanism, attention_layer_size=n_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c247aa54-73c0-4402-b6ec-341b9edbb241",
   "metadata": {},
   "source": [
    "We simply wrap the decoder cell in an `AttentionWrapper`, and we provide the desired attention mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ace679-30c9-4091-813e-f54bd163469d",
   "metadata": {},
   "source": [
    "### Visual Attention"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
